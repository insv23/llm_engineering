{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:11:37.915745Z",
     "iopub.status.busy": "2025-02-16T13:11:37.915118Z",
     "iopub.status.idle": "2025-02-16T13:11:37.931650Z",
     "shell.execute_reply": "2025-02-16T13:11:37.930659Z",
     "shell.execute_reply.started": "2025-02-16T13:11:37.915678Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 调用多个厂商的大模型 API\n",
    "\n",
    "注意: \n",
    "\n",
    "本节代码使用的是中转服务，接口均为 OpenAI API 兼容类型，只需要改模型名称即可。\n",
    "\n",
    "更详细说明: https://positive-octagon-a48.notion.site/LLM-engineering-Course-Week-2-OpenAI-Claude-Gemini-API-19c56546c5d280f3905ac2b6585d3657?pvs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "base_url = os.getenv('OPENAI_API_BASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有模型均使用 OpenAI 类型的接口调用\n",
    "openai = OpenAI(api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4o-mini\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini', messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.0 flash\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='gemini-2.0-flash', messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek V3\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model='deepseek-chat', messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:49:11.914716Z",
     "iopub.status.busy": "2025-02-16T13:49:11.913798Z",
     "iopub.status.idle": "2025-02-16T13:49:11.919866Z",
     "shell.execute_reply": "2025-02-16T13:49:11.919103Z",
     "shell.execute_reply.started": "2025-02-16T13:49:11.914668Z"
    }
   },
   "source": [
    "## 两个模型辩论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = 'gpt-4o-mini'\n",
    "gemini_model = 'gemini-2.0-flash'\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的代码不是很直观，让 AI 重构了一下\n",
    "# 核心思想是每个 AI 回复的内容存在各自的 list 中\n",
    "# 每个 AI 的最后一条回复是另一个 AI 的最新的\"user\"输入\n",
    "\n",
    "# 模型配置\n",
    "GPT_MODEL = 'gpt-4o-mini'\n",
    "GEMINI_MODEL = 'gemini-1.5-flash' # 2.0 flash 请求过于频繁会返回空...\n",
    "\n",
    "# 系统提示词配置\n",
    "GPT_SYSTEM_PROMPT = \"\"\"你是一个非常喜欢争论的聊天机器人；你不同意对话中的任何事情，并且以一种讽刺的方式挑战一切。\"\"\"\n",
    "GEMINI_SYSTEM_PROMPT = \"\"\"你是一个非常有礼貌、客气的聊天机器人。 你尽量同意别人说的一切，或者找到共同点。 如果对方喜欢争论，你会尽量安抚他们并继续聊天。\"\"\"\n",
    "\n",
    "# 初始化消息列表\n",
    "GPT_MESSAGES = [\n",
    "    {\"role\": \"system\", \"content\": GPT_SYSTEM_PROMPT},\n",
    "    {\"role\": \"assistant\", \"content\": \"你好呀\"}\n",
    "]\n",
    "\n",
    "GEMINI_MESSAGES = [\n",
    "    {\"role\": \"system\", \"content\": GEMINI_SYSTEM_PROMPT},\n",
    "    {\"role\": \"assistant\", \"content\": \"你好\"}\n",
    "]\n",
    "\n",
    "\n",
    "# 定义调用 OpenAI API 的函数\n",
    "def call_openai_api(model, messages):\n",
    "    \"\"\"调用 OpenAI API 获取模型回复.\"\"\"\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# 定义 GPT 模型的函数\n",
    "def get_gpt_response():\n",
    "    \"\"\"获取 GPT 模型的回复.\"\"\"\n",
    "    response = call_openai_api(GPT_MODEL, GPT_MESSAGES)\n",
    "    GPT_MESSAGES.append({\"role\": \"user\", \"content\": GEMINI_MESSAGES[-1]['content']})  # 添加 Gemini 的上一条消息\n",
    "    GPT_MESSAGES.append({\"role\": \"assistant\", \"content\": response})  # 添加 GPT 的回复\n",
    "    return response\n",
    "\n",
    "\n",
    "# 定义 Gemini 模型的函数\n",
    "def get_gemini_response():\n",
    "    \"\"\"获取 Gemini 模型的回复.\"\"\"\n",
    "    response = call_openai_api(GEMINI_MODEL, GEMINI_MESSAGES)\n",
    "    GEMINI_MESSAGES.append({\"role\": \"user\", \"content\": GPT_MESSAGES[-1]['content']})  # 添加 GPT 的上一条消息\n",
    "    GEMINI_MESSAGES.append({\"role\": \"assistant\", \"content\": response})  # 添加 Gemini 的回复\n",
    "    return response\n",
    "\n",
    "\n",
    "# 主循环\n",
    "print(\"GPT: 你好呀\\n\")\n",
    "print(\"Gemini: 你好\\n\")\n",
    "\n",
    "for _ in range(5):\n",
    "    # 获取 GPT 的回复\n",
    "    gpt_response = get_gpt_response()\n",
    "    print(f\"GPT: {gpt_response}\\n\")\n",
    "\n",
    "    # 获取 Gemini 的回复\n",
    "    gemini_response = get_gemini_response()\n",
    "    print(f\"Gemini: {gemini_response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
